{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Clipper PDF and add to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trips(filename):\n",
    "    # read first page\n",
    "    tables = camelot.read_pdf(filename,\n",
    "                 pages='1',\n",
    "                 flavor='stream',\n",
    "                 table_areas=['0,500,800,100'])\n",
    "\n",
    "    df_import = tables[0].df\n",
    "    df_import.columns = df_import.iloc[0].str.title()\n",
    "    df_import = df_import[1:]\n",
    "    \n",
    "    # check if more than one page\n",
    "    with open(filename, 'rb') as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        pages = reader.numPages\n",
    "\n",
    "    # read next pages if they exist\n",
    "    if pages > 1:\n",
    "        tables = camelot.read_pdf(filename,\n",
    "                                pages='2-end',\n",
    "                                flavor='stream',\n",
    "                                table_areas=['0,560,800,90'])\n",
    "\n",
    "        for i in range(len(tables)):\n",
    "            next_page = tables[i].df\n",
    "            next_page.columns = next_page.iloc[0].str.title()\n",
    "            next_page = next_page[1:]\n",
    "            df_import = pd.concat([df_import, next_page])\n",
    "            # camelot.plot(tables[i], kind='contour').show() # to check table_areas \n",
    "\n",
    "    # clean up\n",
    "    return df_import.reset_index(drop=True).replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(df_import):\n",
    "       df_import.loc[df_import['Location'] == 'ACT bus', 'Category'] = 'AC Transit'\n",
    "       df_import.loc[df_import['Transaction Type'] == 'Dual-tag entry transaction, no fare deduction', 'Category'] = 'BART Entrance'\n",
    "       df_import.loc[df_import['Transaction Type'] == 'Dual-tag exit transaction, fare payment', 'Category'] = 'BART Exit'\n",
    "       df_import.loc[df_import['Route'] == 'CC60', 'Category'] = 'Cable Car'\n",
    "       df_import.loc[(df_import['Transaction Type'] == 'Dual-tag entry transaction, maximum fare deducted (purse debit)') &\n",
    "              (df_import['Route'].isna()), 'Category'] = 'Caltrain Entrance'\n",
    "       df_import.loc[(df_import['Transaction Type'] == 'Dual-tag exit transaction, fare adjustment (purse rebate)') &\n",
    "              (df_import['Route'].isna()), 'Category'] = 'Caltrain Exit'\n",
    "       df_import.loc[(df_import['Transaction Type'] == 'Dual-tag entry transaction, maximum fare deducted (purse debit)') &\n",
    "              (df_import['Route'] == 'FERRY'), 'Category'] = 'Ferry Entrance'\n",
    "       df_import.loc[(df_import['Transaction Type'] == 'Dual-tag exit transaction, fare adjustment (purse rebate)') &\n",
    "              (df_import['Route'] == 'FERRY'), 'Category'] = 'Ferry Exit'\n",
    "       df_import.loc[df_import['Location'].str[-5:] == '(GGF)', 'Category'] = 'Ferry Exit'\n",
    "       df_import.loc[df_import['Location'] == 'SFM bus', 'Category'] = 'Muni Bus'\n",
    "       df_import.loc[df_import['Route'] == 'NONE', 'Category'] = 'Muni Metro'\n",
    "       df_import.loc[df_import['Location'] == 'SAM bus', 'Category'] = 'SamTrans'\n",
    "       df_import.loc[(df_import['Transaction Type'] == 'Threshold auto-load at a TransLink Device') |\n",
    "              (df_import['Transaction Type'] == 'Add value at TOT or TVM'), 'Category'] = 'Reload'\n",
    "\n",
    "       return df_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(df_import):\n",
    "    df_import['Transaction Date'] = pd.to_datetime(df_import['Transaction Date'])\n",
    "    \n",
    "    for col in ['Debit', 'Credit', 'Balance']:\n",
    "        df_import[col] = df_import[col].str.replace('$', '').astype(float)\n",
    "    \n",
    "    return df_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/04/3nd376_s4cg9q732ybychj4w0000gn/T/ipykernel_37740/194562920.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_import['Transaction Date'] = pd.to_datetime(df_import['Transaction Date'])\n"
     ]
    }
   ],
   "source": [
    "data_k = pd.read_csv('data_k.csv', parse_dates=['Transaction Date'])\n",
    "df = categorize(clean_up(get_trips('raw_data/rideHistory_1202425091-4.pdf')))\n",
    "df = pd.concat([df, data_k]).sort_values('Transaction Date', ascending=False).reset_index(drop=True)\n",
    "df.to_csv('data_k.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
